{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346b453a",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e582c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy requests nlpaug\n",
    "#!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece\n",
    "#!pip install nltk>=3.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa063920",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c3539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024751a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmenter(action):\n",
    "    return naw.ContextualWordEmbsAug(model_path='dbmdz/bert-base-turkish-cased', action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1204ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ƒ∞la√ß kaydetmek istiyorum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"insert\"\n",
    "for i in range(5):\n",
    "    augmented_text = augmenter(action).augment(text)\n",
    "    print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"substitute\"\n",
    "for i in range(5):\n",
    "    augmented_text = augmenter(action).augment(text)\n",
    "    print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8058f",
   "metadata": {},
   "source": [
    "## Easy Data Augmentation (EDA) üöú\n",
    "\n",
    "Please find here the model used below -> https://github.com/akoksal/Turkish-Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7e1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c997138",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "Use regular expression for removing punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4eabc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5735764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('embedding_model_tr', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a2c6ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Kar≈üƒ±mda beni ciddi ciddi s√ºzen, k√º√ß√ºk, e≈üi g√∂r√ºlmedik biri duruyordu.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9624a4",
   "metadata": {},
   "source": [
    "### Synonym Replacement\n",
    "Select a word that is not a stop words and replace it its synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da3e5018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kar≈üƒ±mda',\n",
       " 'beni',\n",
       " 'ciddi',\n",
       " 'ciddi',\n",
       " 's√ºzen',\n",
       " 'k√º√ß√ºk',\n",
       " 'e≈üi',\n",
       " 'g√∂r√ºlmedik',\n",
       " 'biri',\n",
       " 'duruyordu']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = re.sub(r'[^\\w\\s]', '', sample_sentence).split(\" \")\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ed146d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    random_idx = randint(0, len(tokenized_sentence) - 1)\n",
    "    random_word = tokenized_sentence[random_idx]\n",
    "    # there could be a word not included in dictionary if so, choose another.\n",
    "    try:\n",
    "        synonym_word = word_vectors.similar_by_word(random_word)[0][0]\n",
    "        break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2dfc5b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ciddi', '≈üiddetli')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_word, synonym_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fdac4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_tokenized_sentence = tokenized_sentence.copy()\n",
    "replaced_tokenized_sentence[random_idx] = synonym_word\n",
    "augmented_sentence = \" \".join(replaced_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8cb69b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Kar≈üƒ±mda beni ciddi ciddi s√ºzen, k√º√ß√ºk, e≈üi g√∂r√ºlmedik biri duruyordu.',\n",
       " 'Kar≈üƒ±mda beni ciddi ≈üiddetli s√ºzen k√º√ß√ºk e≈üi g√∂r√ºlmedik biri duruyordu')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence, augmented_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf107ca",
   "metadata": {},
   "source": [
    "## Random Insertion\n",
    "Find a random synonym of a random n words in text date and insert into a random place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9d8fab3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kar≈üƒ±mda',\n",
       " 'beni',\n",
       " 'ciddi',\n",
       " 'ciddi',\n",
       " 's√ºzen',\n",
       " 'k√º√ß√ºk',\n",
       " 'e≈üi',\n",
       " 'g√∂r√ºlmedik',\n",
       " 'biri',\n",
       " 'duruyordu']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = re.sub(r'[^\\w\\s]', '', sample_sentence).split(\" \")\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08f523",
   "metadata": {},
   "source": [
    "### Select a random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e45457f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = randint(0, len(tokenized_sentence) - 1)\n",
    "random_word = tokenized_sentence[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "36b27820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biri'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "60981b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'birisi'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_word = word_vectors.similar_by_word(random_word)[0][0]\n",
    "similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8880f20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kar≈üƒ±mda',\n",
       " 'beni',\n",
       " 'ciddi',\n",
       " 'ciddi',\n",
       " 's√ºzen',\n",
       " 'k√º√ß√ºk',\n",
       " 'e≈üi',\n",
       " 'birisi',\n",
       " 'g√∂r√ºlmedik',\n",
       " 'biri',\n",
       " 'duruyordu']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence.insert(randint(0, len(tokenized_sentence)), similar_word)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc987c",
   "metadata": {},
   "source": [
    "## Random Swap\n",
    "Randomly select n words in the text and swap the positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a53e928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kar≈üƒ±mda',\n",
       " 'beni',\n",
       " 'ciddi',\n",
       " 'ciddi',\n",
       " 's√ºzen',\n",
       " 'k√º√ß√ºk',\n",
       " 'e≈üi',\n",
       " 'g√∂r√ºlmedik',\n",
       " 'biri',\n",
       " 'duruyordu']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = re.sub(r'[^\\w\\s]', '', sample_sentence).split(\" \")\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a5b70",
   "metadata": {},
   "source": [
    "#### Find 2 random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b800980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    random_idx1 = randint(0, len(tokenized_sentence) - 1)\n",
    "    random_word1 = tokenized_sentence[random_idx1]\n",
    "\n",
    "    random_idx2 = randint(0, len(tokenized_sentence) - 1)\n",
    "    if not random_idx1 == random_idx2:\n",
    "        random_word2 = tokenized_sentence[random_idx2]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d2c0187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('beni', 'ciddi', 1, 3)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_word1, random_word2, random_idx1, random_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "37db038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_swapped_tokenized_sentence = []\n",
    "for idx, i in enumerate(tokenized_sentence):\n",
    "    if idx == random_idx1:\n",
    "        random_swapped_tokenized_sentence.append(random_word2)\n",
    "    elif idx == random_idx2:\n",
    "        random_swapped_tokenized_sentence.append(random_word1)\n",
    "    else:\n",
    "        random_swapped_tokenized_sentence.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d0766b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kar≈üƒ±mda', 'ciddi', 'ciddi', 'beni', 's√ºzen', 'k√º√ß√ºk', 'e≈üi', 'g√∂r√ºlmedik', 'biri', 'duruyordu']\n",
      "['Kar≈üƒ±mda', 'beni', 'ciddi', 'ciddi', 's√ºzen', 'k√º√ß√ºk', 'e≈üi', 'g√∂r√ºlmedik', 'biri', 'duruyordu']\n"
     ]
    }
   ],
   "source": [
    "print(random_swapped_tokenized_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27de07",
   "metadata": {},
   "source": [
    "## Random Deletion\n",
    "Randomly select n words and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8a4ff4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_idx = randint(0, len(tokenized_sentence) - 1)\n",
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0a0385ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biri'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_word = tokenized_sentence[random_idx]\n",
    "random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "26804d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kar≈üƒ±mda', 'beni', 'ciddi', 'ciddi', 's√ºzen', 'k√º√ß√ºk', 'e≈üi', 'g√∂r√ºlmedik', 'biri', 'duruyordu']\n",
      "['Kar≈üƒ±mda', 'beni', 'ciddi', 'ciddi', 's√ºzen', 'k√º√ß√ºk', 'e≈üi', 'g√∂r√ºlmedik', 'duruyordu']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence)\n",
    "tokenized_sentence.pop(random_idx)\n",
    "print(tokenized_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
